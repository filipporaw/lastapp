<!DOCTYPE html>
<html lang="it">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>GPT, Generazione Pericolosa di Testo</title>
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:ital,wght@0,400;0,700;1,400;1,700&display=swap"
        rel="stylesheet">
</head>
<body>
  <div class="container">
    <header>
      <div class="header-top">
        <h1><a href="../index.html">filippo---raw</a></h1>
        <div class="switch-wrapper">
          <label class="switch">
            <input type="checkbox" id="dark-toggle">
            <span class="slider"></span>
          </label>
        </div>
      </div>
    </header>

    <main>
      <p><a href="../index.html">← Torna alla home</a></p>

      <article>
        <h2>GPT, Generazione Pericolosa di Testo</h2>
        <p class="post-date">Pubblicato il 2 Settembre 2025</p>

        <p>
          Oggigiorno <strong>strumenti AI</strong>, LLM (large language model) e transformer sono
          utilizzati da chiunque, sia con versioni gratuite sia con piani a pagamento. Peccato che
          manchi una cosa fondamentale: l’<strong>AI literacy</strong>, l’alfabetizzazione
          sull’intelligenza artificiale, cioè la capacità di usare questi strumenti in modo
          <em>competente, efficace e responsabile</em>.
        </p>

        <p>
          È uscito un recente sondaggio di WalkMe<a href="https://www.walkme.com/news-releases/employees-left-behind-in-workplace-ai-boom-new-walkme-survey-finds/?tabId=company-news"
          target="_blank">^1</a> che rivela che oltre l’80 % dei dipendenti ammette di
          utilizzare software AI non approvati dalle policy aziendali, esponendo i dati sensibili
          a diversi rischi.  Il campione è ristretto, ma per esperienza personale ritengo
          queste percentuali realistiche.
        </p>

        <hr>

        <h4>Cos’è lo <em>Shadow AI</em>?</h4>
        <p>
          Immagina di essere a lavoro, aprire il tuo laptop, andare su ChatGPT e cominciare a
          usarlo per scrivere email, analizzare dati o creare contenuti. Lo fai perché è veloce e
          comodo, ma <strong>senza chiedere il permesso al reparto IT né rispettare le policy
          aziendali</strong>. Questo è lo <strong><em>Shadow AI</em></strong>: l’uso di applicazioni o modelli di
          intelligenza artificiale al di fuori del controllo ufficiale dell’organizzazione. <a href="https://cloudsecurityalliance.org/blog/2025/03/04/ai-gone-wild-why-shadow-ai-is-your-it-team-s-worst-nightmare"
          target="_blank">^2</a> 
          <a href="https://cloudsecurityalliance.org/blog/2025/03/04/ai-gone-wild-why-shadow-ai-is-your-it-team-s-worst-nightmarehttps://cloudsecurityalliance.org/blog/2025/03/04/ai-gone-wild-why-shadow-ai-is-your-it-team-s-worst-nightmare
"
          target="_blank">^3</a> 
        </p>
      
        <hr>

        <h4>Perché è un problema?</h4>
        <p><strong>Dati sensibili</strong> – Quando i dipendenti caricano documenti, email o altri
        contenuti su servizi AI esterni, spesso questi dati viaggiano senza crittografia. Le
        informazioni finiscono sui server del provider, fuori dal controllo dell’azienda,
        creando una vulnerabilità sfruttabile da concorrenti o attori malevoli.</p>

        <p><strong>Violazione del GDPR</strong> – Il regolamento richiede che i dati personali siano
        trattati solo con una base legale valida e, quando necessario, rimangano all’interno
        dell’UE. Lo <em>Shadow AI</em> può trasferire dati a server extra‑UE o elaborarli senza
        consenso, esponendo l’impresa a multe proporzionali al fatturato e a danni reputazionali.</p>

        <p><strong>Mancanza di audit</strong> – Nessuno sa chi ha attivato quale modello, quando
        e con quali dati. Senza tracciabilità è impossibile ricostruire incidenti o dimostrare
        conformità durante un audit interno o esterno.</p>

        <p><strong>Rischio Copyright</strong> – Molti modelli sono addestrati su dati non
        licenziati. Usarli per produrre contenuti aziendali può esporre l’organizzazione a cause
        per violazione di diritti di proprietà intellettuale.</p>

        <p><strong>Bias</strong> – Modelli non verificati possono incorporare pregiudizi
        presenti nei dati di addestramento, portando a decisioni di business errate o
        discriminazioni involontarie.</p>

        <hr>

        <h4>Il quadro normativo</h4>
        <ul>
          <li>
            <strong>Leggi nazionali</strong> – Il Codice della privacy italiano prevede sanzioni
            severe per il trattamento illecito di dati sanitari, bancari o giudiziari.
            (<a href="https://www.garanteprivacy.it/codice" target="_blank">Garante Privacy</a>)
          </li>
          <li>
            <strong>GDPR (Art. 32)</strong> – Richiede misure tecniche e organizzative adeguate
            per proteggere i dati personali. Inviare informazioni a servizi AI non certificati è
            una violazione diretta.
            (<a href="https://www.garanteprivacy.it/regolamentoue" target="_blank">Garante GDPR</a>)
          </li>
          <li>
            <strong>AI Act dell’UE</strong> – Entrerà in vigore entro il 2026, fornendo il
            primo quadro giuridico completo sull’intelligenza artificiale e imponendo requisiti
            di trasparenza, robustezza e governance.
            (<a href="https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai"
                target="_blank">EU Digital Strategy</a>)
          </li>
        </ul>

        <hr>

        <h4>Come ridurre il rischio</h4>
        <ol>
          <li>
            <strong>Definisci una policy chiara</strong> – Stabilisci quali tool AI sono
            consentiti, dove devono essere ospitati e quali tipologie di dati possono essere
            elaborati. <a href="<a href="<a href="https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai"
                target="_blank">^4</a>)
          </li>
          <li>
            <strong>Adotta soluzioni “trusted AI”</strong> – Scegli piattaforme AI interne o
            versioni enterprise (es. ChatGPT Enterprise) che trattano i dati in modo
            conforme alle policy aziendali.
          </li>
          <li>
            <strong>Formazione degli utenti</strong> – Corsi su AI literacy, GDPR, data
            leakage e conseguenze legali dell’uso non controllato.
          </li>
          <li>
            <strong>Monitora i flussi di dati</strong> – Implementa logging e monitoraggio delle
            chiamate API verso servizi esterni; registra chi, cosa e quando invia informazioni a
            piattaforme AI.
          </li>
        </ol>

        <hr>

        <h4>Altri punti</h4>
        <p><strong>Esempi tipici di utilizzo non autorizzato</strong></p>
        <ul>
          <li>Chatbot integrati in Slack o Teams per rispondere a richieste dei clienti senza
              passare per l’help‑desk aziendale.</li>
          <li>Tool di trascrizione per sintetizzare meeting registrati.</li>
          <li>Tool di traduzione automatica inseriti in workflow di documentazione o email,
              con dati sensibili che attraversano reti esterne senza crittografia end‑to‑end.</li>
        </ul>

        <p><strong>Settori più colpiti</strong></p>
        <ul>
          <li><strong>Consulenza</strong> – Produzione rapida di presentazioni e report con dati
              sensibili dei clienti.</li>
          <li><strong>Pubblica amministrazione</strong> – Automazione di pratiche burocratiche e
              traduzione di documenti ufficiali.</li>
          <li><strong>Media</strong> – Bozza di articoli con dati personali di fonti o metadati
              sensibili.</li>
          <li><strong>Ricerca e sviluppo</strong> – Scrittura di paper scientifici che potrebbero
              divulgare dati non ancora pubblici.</li>
        </ul>

      </article>
    </main>

    <footer>
      <p>--- 2025 ---</p>
      <p>rawfilippo@gmail.com</p>
    </footer>
  </div>

  <script>
    // Toggle dark mode
    const darkToggle = document.getElementById('dark-toggle');
    if (localStorage.getItem('darkMode') === 'enabled') {
      document.body.classList.add('dark-mode');
      darkToggle.checked = true;
    }

    darkToggle.addEventListener('change', () => {
      if (darkToggle.checked) {
        document.body.classList.add('dark-mode');
        localStorage.setItem('darkMode', 'enabled');
      } else {
        document.body.classList.remove('dark-mode');
        localStorage.setItem('darkMode', 'disabled');
      }
    });
  </script>
</body>
</html>